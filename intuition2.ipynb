{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhIvZX5pl9EtrgHcjP57mH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adgcal/Anjan_collab_R/blob/main/intuition2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thermodynamic models\n",
        "## Evolution\n",
        "Life is a struggle against the tendency of any typical thermodynamic system to approach equilibrium. It does so by being an open system that receives and then absorbs *negative \"entropy\" (order)* from the surrounding. This  nullifies the effect of dissipation, the irreversible processes that destroys order.What Schrodinger was silent about was the mechanistic path of such absorption."
      ],
      "metadata": {
        "id": "-rMdx83ACsrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Darwin versus Boltzmann\n",
        "Here are the two complimentary views posed by Darwin and Boltzmann.\n",
        "- Darwin's theory, from thermodynamic point of view can be looked upon as, natural selection favoring generation of order.\n",
        "- The Boltzmann on the other hand conjectures the relevance of ever increasing disorder in the Universe.\n",
        " - $Schr\\ddot{o}$dinger's$ solution was that in open system, there is a supply of order and a generation of disorder, and the competition of the two makes the living state.\n",
        "\n",
        "# $Schr\\ddot{o}$dinger :\n",
        "\\begin{equation}\n",
        "\\Delta S_{Supply} +\\Delta S_{Interal } \\ge 0\n",
        "\\end{equation}\n",
        "The supply term can be both postive and negative. The internal entropy change is the dissipative part and follows the dictum of the second law (namely, always positive).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rd9G_pxTE5n2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A reference to Boltzman and Darwin\n",
        "\\small\n",
        "* One predicts a generation of disorder other predicts the generation of organized complexity or order. How can we reconcile the two?\n",
        "* How can Boltzmann’s type disappearance of unusual more organised states be reconciled with Darwin’s statistical selection of rare events? Darwin’s theory begins with an assumption of the spontaneous fluctuations of species, which are reinforced and lead to self-organisation and more complexity (more order). So apparently we can say that Darwin's theory is about the generation of order and  Boltzmann's theory is about the disappearance of order ( interestingly, the former was a big admirer of the latter)\n",
        "* Boltzmann's tomb\n",
        "$S=k. log W$\n",
        "\n",
        "## Cauchy's theorem\n",
        "\n",
        "Cachy's theorem that if\n",
        "\\begin{equation}\n",
        " f : R->R , f(x)+f(y)=f(x+y), f(x)=x.f(1)\n",
        "\\end{equation}\n",
        "  \n",
        "\\begin{equation}\n",
        "S=S_1+S_2  \n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "W=W_1.W_2\n",
        "\\end{equation}\n",
        "\n",
        "Equations are related by the physical concept that entropy is a functional way of arranging a set of states W.\n",
        "\\begin{equation}\n",
        "S=F(W)\n",
        "\\end{equation}\n",
        "\n",
        "Putting equations (2)-(4) together we find that\n",
        "\\begin{equation} F(W_1)+F(W_2)=F(W_1.W_2)\\end{equation}\n",
        "\\textbf{Problem:}  Use Cachy's functional equation to prove that\n",
        "\\begin{equation}  \n",
        "F(u)+F(v) =F(uv)\n",
        "\\end{equation}\n",
        "Substituting,\n",
        "\\begin{equation}  \n",
        " F() \\equiv k\\cdot log()\n",
        "\\end{equation}\n",
        "equation (6) is satisfied and we have,\n",
        "\\begin{equation}\n",
        " F(W)=k.log(W),\n",
        " \\end{equation}\n"
      ],
      "metadata": {
        "id": "bLzySJvOG4QJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Can Entropy be complex ?\n",
        "\n",
        "Complex entropy, also known as differential entropy or continuous entropy, is a concept from information theory that extends the notion of entropy to continuous probability distributions. While traditional entropy measures the amount of uncertainty or randomness in a discrete probability distribution, complex entropy deals with continuous variables.\n",
        "\n",
        "In the context of continuous probability distributions, complex entropy is defined as the negative of the integral of the probability density function (PDF) multiplied by the natural logarithm of the PDF. Mathematically, for a continuous random variable X with PDF f(x), the complex entropy H(X) is given by:\n",
        "\n",
        "$H(X) = -\\int f(x) * log(f(x)) dx$\n",
        "\n",
        "It's worth noting that complex entropy is not always defined for all continuous probability distributions. Some distributions may have infinite or undefined complex entropy. In such cases, alternative measures like Renyi entropy or Tsallis entropy can be used.\n",
        "\n",
        "Complex entropy has applications in various fields, including signal processing, data compression, and statistical inference. It provides a measure of the uncertainty or amount of information carried by continuous random variables and can be used to analyze and compare different continuous probability distributions."
      ],
      "metadata": {
        "id": "JG9i_GTSG72z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Elements of Information Theory\" by Thomas M. Cover and Joy A. Thomas.\n",
        "\"Information Theory, Inference, and Learning Algorithms\" by David MacKay.\n",
        "\"Information Theory and Statistics\" by Solomon Kullback and Richard A. Leibler.\n",
        "\"Probability and Random Processes\" by Geoffrey Grimmett and David Stirzaker."
      ],
      "metadata": {
        "id": "1_djVBj3rJET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Simpler Account for Complex Entropy\n",
        "The Boltzman law can be written as:\n",
        "\\begin{equation}\n",
        "S = k_B\\cdot log(P)\n",
        "\\end{equation}\n",
        "\n",
        "We can now write the law in an inverse form:\n",
        "\\begin{equation}\n",
        "P= e^{S -2\\pi\\cdot j \\cdot n}\n",
        "\\end{equation}\n",
        "where, n is any integer.\n",
        "\n",
        "We can rewrite the above equation as:\n",
        "\\begin{equation}\n",
        "S/k_B = log(P) + 2\\pi\\cdot j \\cdot n\n",
        "\\end{equation}\n",
        "\n",
        "Now we can conceptualize three systems with equal P values but having different n values. The difference of entropy between such systems will be an imaginary entity:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta S/k_B= 2\\pi. j. n\n",
        "\\end{equation}\n",
        "where , n is an integer.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VujLsUMpr2q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information and Intuition\n",
        "\n",
        "The brief analysis we have outlined can be summarised as follows:\n",
        "- The entropy which is akin to Shannon entropy may have real and imaginary parts.\n",
        "- While the real part corresponds to information how to interpret the imaginary part (which is also called the phase Shannon entropy).\n",
        "- We may interpret  this **phase shannon entropy** as intution.\n",
        "- A surprising outcome of the  said description is that while for a fiven thermodynamic system the **informtion** is unique we can have multiple intutuitive guess about the same same system (technically speaking, the existence of multiplicity of phase Shannon entropy us an eventuality).\n",
        "REF:Complex entropy and resultant information measures,J Math Chem (2016) 54:1777-1782 DOI 10.1007/s10910-016-0651-6."
      ],
      "metadata": {
        "id": "Q3i1bEG-YW_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complex entropy versus the q-entropy\n",
        "A more generalized way of expressing the Boltzmann-Gibbs-Shannon entropy is considering the entropy as an expectation for $log(1/p_i)$.\n",
        "$S_{BGS}=\\left\\langle ln(1/p_i)   \\right\\rangle$.  A new form of entropy based on the natural logarithm was proposed to take into account incomplete information. The entropy is then defined by @radhakrishnan2014fractional.\n",
        "\n",
        "\\begin{equation}\n",
        "S_{BGS}=\\left\\langle ln(1/p_i)   \\right\\rangle= \\Sigma_i p_i^q ln(1/p_i)\n",
        "\\end{equation}\n",
        "\n",
        "If we assume that complex q-entropy always assumes a real value the simplification we obtain for a two state system is as follows :\n",
        "\n",
        "\\begin{equation}\n",
        "\\Sigma_i 2\\pi\\cdot n_i\\cdot j\\cdot p_i^q=0\n",
        "\\end{equation}\n",
        "\n",
        "For a two state system this ammounts to:\n",
        "\\begin{equation}\n",
        "\\frac{p1}{p2}=\\left(- \\frac{n_2}{n_1} \\right)^{1/q}\n",
        "\\label{eq:twostates}\n",
        "\\end{equation}\n",
        "\n",
        "In the equation (\\ref{eq:twostates}) one of the important feature in the states '1'  and '2' will be coupled by is either  $sgn(n1)+sgn(n2)=0$ or   $1/q = 2k,k=1,2,..$ so that the probability ratio assumes a positive value. It may be noted that the q-expectation value describes some \"incompleteness\" of small or complex systems. Therefore, a selection rule on \"q\" indicated some restrictions to the nature of complex behaviour that may emerge. The take home message in this simple excercise is that we can observe very large compex systems but even in such systems there is some restrictions in the mutual interaction. This remain valid when at the micron scale  cellular clusters are formed or at the nanoscale nanoclusters are formed, or eben at he sub nanolebel when the small molecules like water gets clustered due to hydrogen bond formation.\n"
      ],
      "metadata": {
        "id": "SMlZ44KCsSWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a two state system this ammounts to:\n",
        "\\begin{equation}\n",
        "\\frac{p1}{p2}=\\left(- \\frac{n_2}{n_1} \\right)^{1/q}\n",
        "\\end{equation}\n",
        "\n",
        "In the equation (\\ref{eq:twostates}) one of the important feature in the states '1'  and '2' will be coupled by is either  $sgn(n1)+sgn(n2)=0$ or   $1/q = 2k,k=1,2,..$ so that the probability ratio assumes a positive value. It may be noted that the q-expectation value describes some \"incompleteness\" of small or complex systems. Therefore, a selection rule on \"q\" indicated some restrictions to the nature of complex behaviour that may emerge. The take home message in this simple excercise is that we can observe very large compex systems but even in such systems there is some restrictions in the mutual interaction. This remain valid when at the micron scale  cellular clusters are formed or at the nanoscale nanoclusters are formed, or eben at he sub nanolebel when the small molecules like water gets clustered due to hydrogen bond formation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0pN6eSodsywU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:A Fractional Entropy in Fractal Phase Space: Properties and Characterization\n",
        "International Journal of Statistical Mechanics Volume 2014, Article ID 460364, 16 pages http://dx.doi.org/10.1155/2014/460364\n"
      ],
      "metadata": {
        "id": "vJSLsu_ttXmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "The moot question that was raised in course of this presentaion is whether intuition is a metaphysical entitly thay only strikes certain individuals like a 'lightning' and is intrinscically noncomputable.\n",
        "\n",
        "The complex entropy interpretaion of intuition in a sense hits at a complmentary nature of information and intuition and a non-metaphysical existence of the intuitive component.\n",
        "\n",
        "The multivaluedness of the Shannon phase entropy  might enable us a new paradigm of quantum superposition like principle."
      ],
      "metadata": {
        "id": "C6uW3EIAcKJh"
      }
    }
  ]
}